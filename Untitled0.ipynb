{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD2VLzazf0Xc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import linear, relu, sigmoid\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('./deeplearning.mplstyle')\n",
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
        "tf.autograph.set_verbosity(0)\n",
        "from public_tests import * \n",
        "from autils import *\n",
        "from lab_utils_softmax import plt_softmax\n",
        "np.set_printoptions(precision=2)\n",
        "plt_act_trio()\n",
        "def my_softmax(z):   \n",
        "    ez = np.exp(z)              \n",
        "    a = ez/np.sum(ez)\n",
        "    return a\n",
        "z = np.array([1., 2., 3., 4.])\n",
        "a = my_softmax(z)\n",
        "atf = tf.nn.softmax(z)\n",
        "print(f\"my_softmax(z):         {a}\")\n",
        "print(f\"tensorflow softmax(z): {atf}\")  \n",
        "test_my_softmax(my_softmax)\n",
        "plt.close(\"all\")\n",
        "plt_softmax(my_softmax)\n",
        "X, y = load_data()\n",
        "print ('The first element of X is: ', X[0])\n",
        "print ('The first element of y is: ', y[0,0])\n",
        "print ('The last element of y is: ', y[-1,0])\n",
        "print ('The shape of X is: ' + str(X.shape))\n",
        "print ('The shape of y is: ' + str(y.shape))\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "m, n = X.shape\n",
        "fig, axes = plt.subplots(8,8, figsize=(5,5))\n",
        "fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) #[left, bottom, right, top]\n",
        "#fig.tight_layout(pad=0.5)\n",
        "widgvis(fig)\n",
        "for i,ax in enumerate(axes.flat):\n",
        "    # Select random indices\n",
        "    random_index = np.random.randint(m)   \n",
        "    # Select rows corresponding to the random indices and\n",
        "    # reshape the image\n",
        "    X_random_reshaped = X[random_index].reshape((20,20)).T   \n",
        "    # Display the image\n",
        "    ax.imshow(X_random_reshaped, cmap='gray')   \n",
        "    # Display the label above the image\n",
        "    ax.set_title(y[random_index,0])\n",
        "    ax.set_axis_off()\n",
        "    fig.suptitle(\"Label, image\", fontsize=14)\n",
        "tf.random.set_seed(1234) # for consistent results\n",
        "model = Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(400,)),\n",
        "        Dense(25, activation = 'relu',   name = \"L1\"),\n",
        "        Dense(15, activation = 'relu', name = \"L2\"),\n",
        "        Dense(10, activation = 'linear', name =\"L3\"),\n",
        "    ], name = \"my_model\" \n",
        ")\n",
        "model.summary()\n",
        "test_model(model, 10, 400)\n",
        "[layer1, layer2, layer3] = model.layers\n",
        "W1,b1 = layer1.get_weights()\n",
        "W2,b2 = layer2.get_weights()\n",
        "W3,b3 = layer3.get_weights()\n",
        "print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
        "print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\n",
        "print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X,y,\n",
        "    epochs=40\n",
        ")\n",
        "plot_loss_tf(history)\n",
        "image_of_two = X[1015]\n",
        "display_digit(image_of_two)\n",
        "\n",
        "prediction = model.predict(image_of_two.reshape(1,400))\n",
        "\n",
        "print(f\" predicting a Two: \\n{prediction}\")\n",
        "print(f\" Largest Prediction index: {np.argmax(prediction)}\")\n",
        "prediction_p = tf.nn.softmax(prediction)\n",
        "\n",
        "print(f\" predicting a Two. Probability vector: \\n{prediction_p}\")\n",
        "print(f\"Total of predictions: {np.sum(prediction_p):0.3f}\")\n",
        "yhat = np.argmax(prediction_p)\n",
        "\n",
        "print(f\"np.argmax(prediction_p): {yhat}\")\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "m, n = X.shape\n",
        "\n",
        "fig, axes = plt.subplots(8,8, figsize=(5,5))\n",
        "fig.tight_layout(pad=0.13,rect=[0, 0.03, 1, 0.91]) #[left, bottom, right, top]\n",
        "widgvis(fig)\n",
        "for i,ax in enumerate(axes.flat):\n",
        "    # Select random indices\n",
        "    random_index = np.random.randint(m)\n",
        "    X_random_reshaped = X[random_index].reshape((20,20)).T\n",
        "    ax.imshow(X_random_reshaped, cmap='gray')    \n",
        "    # Predict using the Neural Network\n",
        "    prediction = model.predict(X[random_index].reshape(1,400))\n",
        "    prediction_p = tf.nn.softmax(prediction)\n",
        "    yhat = np.argmax(prediction_p)\n",
        "    ax.set_title(f\"{y[random_index,0]},{yhat}\",fontsize=10)\n",
        "    ax.set_axis_off()\n",
        "fig.suptitle(\"Label, yhat\", fontsize=14)\n",
        "plt.show()\n",
        "print( f\"{display_errors(model,X,y)} errors out of {len(X)} images\")"
      ]
    }
  ]
}